{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4b3f5f-b3e4-415c-9461-4149635cb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime.training import artifacts\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477b77e5-c764-424f-be50-0e177d0c1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head self-attention\n",
    "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        # Add and norm\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward network\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        # Add and norm\n",
    "        out2 = self.layer_norm2(out1 + ff_output)\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5a11aa-f242-4efb-b36e-2ad2771598aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example usage of TransformerBlock\n",
    "input_tensor = torch.randn(10, 32, 512)  # (sequence_length, batch_size, embed_size)\n",
    "transformer_block = TransformerBlock(embed_size=512, heads=8)\n",
    "output_tensor = transformer_block(input_tensor)\n",
    "print(output_tensor.shape)  # should print: torch.Size([10, 32, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3c1d1f-79af-432c-a0cb-c39f3499cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = transformer_block(input_tensor)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e704bf-1639-4104-bf45-321ef4c2681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    transformer_block,\n",
    "    input_tensor,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57cc759-a9b8-4f74-bf3a-901fb8591f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = [name for name, param in transformer_block.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in transformer_block.named_parameters() if not param.requires_grad]\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    #loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"Transformer\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca77e01a-0c9f-4eb5-98ce-460f6fc5c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex32x512]\n",
      "  %target[FLOAT, batch_sizex32x512]\n",
      "  %attention.in_proj_weight[FLOAT, 1536x512]\n",
      "  %attention.in_proj_bias[FLOAT, 1536]\n",
      "  %attention.out_proj.weight[FLOAT, 512x512]\n",
      "  %attention.out_proj.bias[FLOAT, 512]\n",
      "  %feed_forward.0.weight[FLOAT, 2048x512]\n",
      "  %feed_forward.0.bias[FLOAT, 2048]\n",
      "  %feed_forward.2.weight[FLOAT, 512x2048]\n",
      "  %feed_forward.2.bias[FLOAT, 512]\n",
      "  %layer_norm1.weight[FLOAT, 512]\n",
      "  %layer_norm1.bias[FLOAT, 512]\n",
      "  %layer_norm2.weight[FLOAT, 512]\n",
      "  %layer_norm2.bias[FLOAT, 512]\n",
      "  %attention.in_proj_weight_grad.accumulation.buffer[FLOAT, 1536x512]\n",
      "  %attention.in_proj_bias_grad.accumulation.buffer[FLOAT, 1536]\n",
      "  %attention.out_proj.weight_grad.accumulation.buffer[FLOAT, 512x512]\n",
      "  %attention.out_proj.bias_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %feed_forward.0.weight_grad.accumulation.buffer[FLOAT, 2048x512]\n",
      "  %feed_forward.0.bias_grad.accumulation.buffer[FLOAT, 2048]\n",
      "  %feed_forward.2.weight_grad.accumulation.buffer[FLOAT, 512x2048]\n",
      "  %feed_forward.2.bias_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %layer_norm1.weight_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %layer_norm1.bias_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %layer_norm2.weight_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %layer_norm2.bias_grad.accumulation.buffer[FLOAT, 512]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %onnx::pow_exponent::3[FLOAT, 1]\n",
      "  %/attention/Concat_6_output_0[INT64, 3]\n",
      "  %/attention/Constant_1_output_0[INT64, scalar]\n",
      "  %/attention/Constant_2_output_0[INT64, scalar]\n",
      "  %/dropout_1/Constant_1_output_0[BOOL, scalar]\n",
      "  %/attention/Constant_4_output_0[INT64, scalar]\n",
      "  %onnx::Unsqueeze_92[INT64, 1]\n",
      "  %/attention/Concat_output_0[INT64, 2]\n",
      "  %/attention/Constant_25_output_0[FLOAT, scalar]\n",
      "  %/attention/Slice_2_output_0[INT64, 0]\n",
      "  %/dropout/Constant_1_output_0[BOOL, scalar]\n",
      "  %/dropout/Constant_output_0[FLOAT, scalar]\n",
      "  %/attention/Concat_5_output_0[INT64, 2]\n",
      "  %/attention/Concat_3_output_0[INT64, 3]\n",
      "  %/attention/Concat_2_output_0[INT64, 3]\n",
      "  %/attention/Constant_12_output_0[INT64, 1]\n",
      "  %/attention/Reshape_output_0[INT64, 1]\n",
      "  %onnx::reducemean_output::6_grad[FLOAT, scalar]\n",
      "  %/attention/MatMul_Grad/A_target_shape[INT64, 2]\n",
      "  %/attention/MatMul_Grad/dY_target_shape[INT64, 2]\n",
      "  %/attention/Add_Grad/ReduceAxes_for_attention.in_proj_bias_grad[INT64, 2]\n",
      "  %/attention/Gemm_Grad/ReduceAxes_for_/attention/Gemm_Grad/dC_reduced[INT64, 1]\n",
      "  %/feed_forward/feed_forward.0/MatMul_Grad/A_target_shape[INT64, 2]\n",
      "  %/feed_forward/feed_forward.0/MatMul_Grad/dY_target_shape[INT64, 2]\n",
      "  %/feed_forward/feed_forward.0/Add_Grad/ReduceAxes_for_feed_forward.0.bias_grad[INT64, 2]\n",
      "  %/feed_forward/feed_forward.2/MatMul_Grad/A_target_shape[INT64, 2]\n",
      "  %/feed_forward/feed_forward.2/MatMul_Grad/dY_target_shape[INT64, 2]\n",
      "  %/feed_forward/feed_forward.2/Add_Grad/ReduceAxes_for_feed_forward.2.bias_grad[INT64, 2]\n",
      "  %OneConstant_Type1[FLOAT, 1]\n",
      ") {\n",
      "  %/feed_forward/feed_forward.0/Transpose_output_0 = Transpose[perm = [1, 0]](%feed_forward.0.weight)\n",
      "  %/attention/Transpose_output_0 = Transpose[perm = [1, 0]](%attention.in_proj_weight)\n",
      "  %/attention/MatMul_output_0 = MatMul(%input, %/attention/Transpose_output_0)\n",
      "  %/attention/Add_output_0 = Add(%attention.in_proj_bias, %/attention/MatMul_output_0)\n",
      "  %/attention/Shape_4_output_0 = Shape(%/attention/Add_output_0)\n",
      "  %/attention/Reshape_2_Grad/x_shape = Shape(%/attention/Add_output_0)\n",
      "  %/attention/Slice_1_output_0 = Slice(%/attention/Shape_4_output_0, %onnx::Unsqueeze_92, %/attention/Reshape_output_0)\n",
      "  %/attention/Concat_1_output_0, %per_input_length = ConcatTraining[axis = 0](%/attention/Slice_1_output_0, %/attention/Concat_output_0, %/attention/Slice_2_output_0)\n",
      "  %/attention/Reshape_2_output_0 = Reshape[allowzero = 0](%/attention/Add_output_0, %/attention/Concat_1_output_0)\n",
      "  %/attention/Unsqueeze_2_output_0 = Unsqueeze(%/attention/Reshape_2_output_0, %onnx::Unsqueeze_92)\n",
      "  %/attention/Transpose_1_output_0 = Transpose[perm = [3, 1, 2, 0, 4]](%/attention/Unsqueeze_2_output_0)\n",
      "  %/attention/Squeeze_1_output_0 = Squeeze(%/attention/Transpose_1_output_0, %/attention/Constant_12_output_0)\n",
      "  %/attention/Gather_4_Grad/I0_shape = Shape(%/attention/Squeeze_1_output_0)\n",
      "  %/attention/Gather_3_Grad/I0_shape = Shape(%/attention/Squeeze_1_output_0)\n",
      "  %/attention/Gather_5_Grad/I0_shape = Shape(%/attention/Squeeze_1_output_0)\n",
      "  %/attention/Gather_5_output_0 = Gather[axis = 0](%/attention/Squeeze_1_output_0, %/attention/Constant_4_output_0)\n",
      "  %/attention/Reshape_5_Grad/x_shape = Shape(%/attention/Gather_5_output_0)\n",
      "  %/attention/Reshape_5_output_0 = Reshape[allowzero = 0](%/attention/Gather_5_output_0, %/attention/Concat_3_output_0)\n",
      "  %/attention/Transpose_3_output_0 = Transpose[perm = [1, 0, 2]](%/attention/Reshape_5_output_0)\n",
      "  %/attention/Gather_3_output_0 = Gather[axis = 0](%/attention/Squeeze_1_output_0, %/attention/Constant_1_output_0)\n",
      "  %/attention/Reshape_3_Grad/x_shape = Shape(%/attention/Gather_3_output_0)\n",
      "  %/attention/Reshape_3_output_0 = Reshape[allowzero = 0](%/attention/Gather_3_output_0, %/attention/Concat_2_output_0)\n",
      "  %/attention/Transpose_2_output_0 = Transpose[perm = [1, 0, 2]](%/attention/Reshape_3_output_0)\n",
      "  %/attention/Mul_1_output_0 = Mul(%/attention/Transpose_2_output_0, %/attention/Constant_25_output_0)\n",
      "  %/attention/Gather_4_output_0 = Gather[axis = 0](%/attention/Squeeze_1_output_0, %/attention/Constant_2_output_0)\n",
      "  %/attention/Reshape_4_Grad/x_shape = Shape(%/attention/Gather_4_output_0)\n",
      "  %/attention/Reshape_4_output_0 = Reshape[allowzero = 0](%/attention/Gather_4_output_0, %/attention/Concat_3_output_0)\n",
      "  %/attention/Transpose_4_output_0 = Transpose[perm = [1, 2, 0]](%/attention/Reshape_4_output_0)\n",
      "  %/attention/MatMul_1_output_0 = MatMul(%/attention/Mul_1_output_0, %/attention/Transpose_4_output_0)\n",
      "  %/attention/Softmax_output_0 = Softmax[axis = -1](%/attention/MatMul_1_output_0)\n",
      "  %/attention/MatMul_2_output_0 = MatMul(%/attention/Softmax_output_0, %/attention/Transpose_3_output_0)\n",
      "  %/attention/Transpose_5_output_0 = Transpose[perm = [1, 0, 2]](%/attention/MatMul_2_output_0)\n",
      "  %/attention/Reshape_6_Grad/x_shape = Shape(%/attention/Transpose_5_output_0)\n",
      "  %/attention/Reshape_6_output_0 = Reshape[allowzero = 0](%/attention/Transpose_5_output_0, %/attention/Concat_5_output_0)\n",
      "  %/attention/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/attention/Reshape_6_output_0, %attention.out_proj.weight, %attention.out_proj.bias)\n",
      "  %/attention/Reshape_7_Grad/x_shape = Shape(%/attention/Gemm_output_0)\n",
      "  %/attention/Reshape_7_output_0 = Reshape[allowzero = 0](%/attention/Gemm_output_0, %/attention/Concat_6_output_0)\n",
      "  %/dropout/Dropout_output_0, %/dropout/Dropout_output_1 = Dropout(%/attention/Reshape_7_output_0, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/Add_output_0 = Add(%input, %/dropout/Dropout_output_0)\n",
      "  %/layer_norm1/Add_1_output_0, %saved_mean, %saved_inv_std_var = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_output_0, %layer_norm1.weight, %layer_norm1.bias)\n",
      "  %/feed_forward/feed_forward.0/MatMul_output_0 = MatMul(%/layer_norm1/Add_1_output_0, %/feed_forward/feed_forward.0/Transpose_output_0)\n",
      "  %/feed_forward/feed_forward.0/Add_output_0 = Add(%feed_forward.0.bias, %/feed_forward/feed_forward.0/MatMul_output_0)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0 = Relu(%/feed_forward/feed_forward.0/Add_output_0)\n",
      "  %/feed_forward/feed_forward.2/Transpose_output_0 = Transpose[perm = [1, 0]](%feed_forward.2.weight)\n",
      "  %/feed_forward/feed_forward.2/MatMul_output_0 = MatMul(%/feed_forward/feed_forward.1/Relu_output_0, %/feed_forward/feed_forward.2/Transpose_output_0)\n",
      "  %/feed_forward/feed_forward.2/Add_output_0 = Add(%feed_forward.2.bias, %/feed_forward/feed_forward.2/MatMul_output_0)\n",
      "  %/dropout_1/Dropout_output_0, %/dropout_1/Dropout_output_1 = Dropout(%/feed_forward/feed_forward.2/Add_output_0, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %/Add_1_output_0 = Add(%/layer_norm1/Add_1_output_0, %/dropout_1/Dropout_output_0)\n",
      "  %output, %saved_mean_token_0, %saved_inv_std_var_token_1 = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_1_output_0, %layer_norm2.weight, %layer_norm2.bias)\n",
      "  %onnx::sub_output::1 = Sub(%output, %target)\n",
      "  %onnx::pow_output::4 = Pow(%onnx::sub_output::1, %onnx::pow_exponent::3)\n",
      "  %onnx::ReduceMean::7_Grad/Sized_X = Size(%onnx::pow_output::4)\n",
      "  %onnx::ReduceMean::7_Grad/Shaped_X = Shape(%onnx::pow_output::4)\n",
      "  %onnx::ReduceMean::7_Grad/Sized_Grad = Size(%onnx::reducemean_output::6_grad)\n",
      "  %onnx::ReduceMean::7_Grad/Scale = Div(%onnx::ReduceMean::7_Grad/Sized_X, %onnx::ReduceMean::7_Grad/Sized_Grad)\n",
      "  %onnx::ReduceMean::7_Grad/Scaled_Grad = Scale[scale_down = 1](%onnx::reducemean_output::6_grad, %onnx::ReduceMean::7_Grad/Scale)\n",
      "  %onnx::pow_output::4_grad = Expand(%onnx::ReduceMean::7_Grad/Scaled_Grad, %onnx::ReduceMean::7_Grad/Shaped_X)\n",
      "  %onnx::Pow::5_Grad/Sub_I1 = Sub(%onnx::pow_exponent::3, %OneConstant_Type1)\n",
      "  %onnx::Pow::5_Grad/Pow_I0 = Pow(%onnx::sub_output::1, %onnx::Pow::5_Grad/Sub_I1)\n",
      "  %onnx::Pow::5_Grad/Mul_Pow_I0_I1 = Mul(%onnx::Pow::5_Grad/Pow_I0, %onnx::pow_exponent::3)\n",
      "  %onnx::sub_output::1_grad = Mul(%onnx::Pow::5_Grad/Mul_Pow_I0_I1, %onnx::pow_output::4_grad)\n",
      "  %output_grad = Identity(%onnx::sub_output::1_grad)\n",
      "  %/Add_1_output_0_grad, %layer_norm2.weight_grad, %layer_norm2.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%output_grad, %/Add_1_output_0, %layer_norm2.weight, %saved_mean_token_0, %saved_inv_std_var_token_1)\n",
      "  %/dropout_1/Dropout_output_0_grad = Identity(%/Add_1_output_0_grad)\n",
      "  %/feed_forward/feed_forward.2/Add_output_0_grad = DropoutGrad(%/dropout_1/Dropout_output_0_grad, %/dropout_1/Dropout_output_1, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %feed_forward.2.bias_grad = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/feed_forward/feed_forward.2/Add_output_0_grad, %/feed_forward/feed_forward.2/Add_Grad/ReduceAxes_for_feed_forward.2.bias_grad)\n",
      "  %/feed_forward/feed_forward.2/MatMul_output_0_grad = Identity(%/feed_forward/feed_forward.2/Add_output_0_grad)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0_grad = FusedMatMul[alpha = 1, transA = 0, transB = 1, transBatchA = 0, transBatchB = 0](%/feed_forward/feed_forward.2/MatMul_output_0_grad, %/feed_forward/feed_forward.2/Transpose_output_0)\n",
      "  %/feed_forward/feed_forward.0/Add_output_0_grad = ReluGrad(%/feed_forward/feed_forward.1/Relu_output_0_grad, %/feed_forward/feed_forward.1/Relu_output_0)\n",
      "  %feed_forward.0.bias_grad = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/feed_forward/feed_forward.0/Add_output_0_grad, %/feed_forward/feed_forward.0/Add_Grad/ReduceAxes_for_feed_forward.0.bias_grad)\n",
      "  %/feed_forward/feed_forward.0/MatMul_output_0_grad = Identity(%/feed_forward/feed_forward.0/Add_output_0_grad)\n",
      "  %/feed_forward/feed_forward.0/MatMul_Grad/dY_reshape_2d = Reshape[allowzero = 0](%/feed_forward/feed_forward.0/MatMul_output_0_grad, %/feed_forward/feed_forward.0/MatMul_Grad/dY_target_shape)\n",
      "  %/feed_forward/feed_forward.0/MatMul_Grad/A_reshape_2d = Reshape[allowzero = 0](%/layer_norm1/Add_1_output_0, %/feed_forward/feed_forward.0/MatMul_Grad/A_target_shape)\n",
      "  %/feed_forward/feed_forward.0/Transpose_output_0_grad = Gemm[alpha = 1, beta = 1, transA = 1, transB = 0](%/feed_forward/feed_forward.0/MatMul_Grad/A_reshape_2d, %/feed_forward/feed_forward.0/MatMul_Grad/dY_reshape_2d)\n",
      "  %feed_forward.0.weight_grad = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.0/Transpose_output_0_grad)\n",
      "  %/feed_forward/feed_forward.2/MatMul_Grad/dY_reshape_2d = Reshape[allowzero = 0](%/feed_forward/feed_forward.2/MatMul_output_0_grad, %/feed_forward/feed_forward.2/MatMul_Grad/dY_target_shape)\n",
      "  %/feed_forward/feed_forward.2/MatMul_Grad/A_reshape_2d = Reshape[allowzero = 0](%/feed_forward/feed_forward.1/Relu_output_0, %/feed_forward/feed_forward.2/MatMul_Grad/A_target_shape)\n",
      "  %/feed_forward/feed_forward.2/Transpose_output_0_grad = Gemm[alpha = 1, beta = 1, transA = 1, transB = 0](%/feed_forward/feed_forward.2/MatMul_Grad/A_reshape_2d, %/feed_forward/feed_forward.2/MatMul_Grad/dY_reshape_2d)\n",
      "  %feed_forward.2.weight_grad = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.2/Transpose_output_0_grad)\n",
      "  %/layer_norm1/Add_1_output_0_grad_1 = Identity(%/Add_1_output_0_grad)\n",
      "  %/layer_norm1/Add_1_output_0_grad_0 = FusedMatMul[alpha = 1, transA = 0, transB = 1, transBatchA = 0, transBatchB = 0](%/feed_forward/feed_forward.0/MatMul_output_0_grad, %/feed_forward/feed_forward.0/Transpose_output_0)\n",
      "  %/layer_norm1/Add_1_output_0_grad = Sum(%/layer_norm1/Add_1_output_0_grad_0, %/layer_norm1/Add_1_output_0_grad_1)\n",
      "  %/Add_output_0_grad, %layer_norm1.weight_grad, %layer_norm1.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/layer_norm1/Add_1_output_0_grad, %/Add_output_0, %layer_norm1.weight, %saved_mean, %saved_inv_std_var)\n",
      "  %/dropout/Dropout_output_0_grad = Identity(%/Add_output_0_grad)\n",
      "  %/attention/Reshape_7_output_0_grad = DropoutGrad(%/dropout/Dropout_output_0_grad, %/dropout/Dropout_output_1, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/attention/Gemm_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_7_output_0_grad, %/attention/Reshape_7_Grad/x_shape)\n",
      "  %/attention/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/attention/Gemm_output_0_grad, %/attention/Gemm_Grad/ReduceAxes_for_/attention/Gemm_Grad/dC_reduced)\n",
      "  %attention.out_proj.bias_grad = Identity(%/attention/Gemm_Grad/dC_reduced)\n",
      "  %attention.out_proj.weight_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/attention/Gemm_output_0_grad, %/attention/Reshape_6_output_0)\n",
      "  %/attention/Reshape_6_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%/attention/Gemm_output_0_grad, %attention.out_proj.weight)\n",
      "  %/attention/Transpose_5_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_6_output_0_grad, %/attention/Reshape_6_Grad/x_shape)\n",
      "  %/attention/MatMul_2_output_0_grad = Transpose[perm = [1, 0, 2]](%/attention/Transpose_5_output_0_grad)\n",
      "  %/attention/Transpose_3_output_0_grad = FusedMatMul[alpha = 1, transA = 1, transB = 0, transBatchA = 0, transBatchB = 0](%/attention/Softmax_output_0, %/attention/MatMul_2_output_0_grad)\n",
      "  %/attention/Reshape_5_output_0_grad = Transpose[perm = [1, 0, 2]](%/attention/Transpose_3_output_0_grad)\n",
      "  %/attention/Gather_5_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_5_output_0_grad, %/attention/Reshape_5_Grad/x_shape)\n",
      "  %/attention/Squeeze_1_output_0_grad_2 = GatherGrad[axis = 0](%/attention/Gather_5_Grad/I0_shape, %/attention/Constant_4_output_0, %/attention/Gather_5_output_0_grad)\n",
      "  %/attention/Softmax_output_0_grad = FusedMatMul[alpha = 1, transA = 0, transB = 1, transBatchA = 0, transBatchB = 0](%/attention/MatMul_2_output_0_grad, %/attention/Transpose_3_output_0)\n",
      "  %/attention/MatMul_1_output_0_grad = SoftmaxGrad_13[axis = -1](%/attention/Softmax_output_0_grad, %/attention/Softmax_output_0)\n",
      "  %/attention/Mul_1_output_0_grad = FusedMatMul[alpha = 1, transA = 0, transB = 1, transBatchA = 0, transBatchB = 0](%/attention/MatMul_1_output_0_grad, %/attention/Transpose_4_output_0)\n",
      "  %/attention/Mul_1_Grad/PreReduceGrad0 = Mul(%/attention/Mul_1_output_0_grad, %/attention/Constant_25_output_0)\n",
      "  %/attention/Transpose_2_output_0_grad = Identity(%/attention/Mul_1_Grad/PreReduceGrad0)\n",
      "  %/attention/Reshape_3_output_0_grad = Transpose[perm = [1, 0, 2]](%/attention/Transpose_2_output_0_grad)\n",
      "  %/attention/Gather_3_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_3_output_0_grad, %/attention/Reshape_3_Grad/x_shape)\n",
      "  %/attention/Squeeze_1_output_0_grad_1 = GatherGrad[axis = 0](%/attention/Gather_3_Grad/I0_shape, %/attention/Constant_1_output_0, %/attention/Gather_3_output_0_grad)\n",
      "  %/attention/Transpose_4_output_0_grad = FusedMatMul[alpha = 1, transA = 1, transB = 0, transBatchA = 0, transBatchB = 0](%/attention/Mul_1_output_0, %/attention/MatMul_1_output_0_grad)\n",
      "  %/attention/Reshape_4_output_0_grad = Transpose[perm = [2, 0, 1]](%/attention/Transpose_4_output_0_grad)\n",
      "  %/attention/Gather_4_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_4_output_0_grad, %/attention/Reshape_4_Grad/x_shape)\n",
      "  %/attention/Squeeze_1_output_0_grad_0 = GatherGrad[axis = 0](%/attention/Gather_4_Grad/I0_shape, %/attention/Constant_2_output_0, %/attention/Gather_4_output_0_grad)\n",
      "  %/attention/Squeeze_1_output_0_grad = Sum(%/attention/Squeeze_1_output_0_grad_0, %/attention/Squeeze_1_output_0_grad_1, %/attention/Squeeze_1_output_0_grad_2)\n",
      "  %/attention/Transpose_1_output_0_grad = Unsqueeze(%/attention/Squeeze_1_output_0_grad, %/attention/Constant_12_output_0)\n",
      "  %/attention/Unsqueeze_2_output_0_grad = Transpose[perm = [3, 1, 2, 0, 4]](%/attention/Transpose_1_output_0_grad)\n",
      "  %/attention/Reshape_2_output_0_grad = Squeeze(%/attention/Unsqueeze_2_output_0_grad, %onnx::Unsqueeze_92)\n",
      "  %/attention/Add_output_0_grad = Reshape[allowzero = 0](%/attention/Reshape_2_output_0_grad, %/attention/Reshape_2_Grad/x_shape)\n",
      "  %attention.in_proj_bias_grad = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/attention/Add_output_0_grad, %/attention/Add_Grad/ReduceAxes_for_attention.in_proj_bias_grad)\n",
      "  %/attention/MatMul_output_0_grad = Identity(%/attention/Add_output_0_grad)\n",
      "  %/attention/MatMul_Grad/dY_reshape_2d = Reshape[allowzero = 0](%/attention/MatMul_output_0_grad, %/attention/MatMul_Grad/dY_target_shape)\n",
      "  %/attention/MatMul_Grad/A_reshape_2d = Reshape[allowzero = 0](%input, %/attention/MatMul_Grad/A_target_shape)\n",
      "  %/attention/Transpose_output_0_grad = Gemm[alpha = 1, beta = 1, transA = 1, transB = 0](%/attention/MatMul_Grad/A_reshape_2d, %/attention/MatMul_Grad/dY_reshape_2d)\n",
      "  %attention.in_proj_weight_grad = Transpose[perm = [1, 0]](%/attention/Transpose_output_0_grad)\n",
      "  %onnx::reducemean_output::6 = ReduceMean[keepdims = 0](%onnx::pow_output::4)\n",
      "  %attention.in_proj_weight_grad.accumulation.out = InPlaceAccumulatorV2(%attention.in_proj_weight_grad.accumulation.buffer, %attention.in_proj_weight_grad, %lazy_reset_grad)\n",
      "  %attention.in_proj_bias_grad.accumulation.out = InPlaceAccumulatorV2(%attention.in_proj_bias_grad.accumulation.buffer, %attention.in_proj_bias_grad, %lazy_reset_grad)\n",
      "  %attention.out_proj.weight_grad.accumulation.out = InPlaceAccumulatorV2(%attention.out_proj.weight_grad.accumulation.buffer, %attention.out_proj.weight_grad, %lazy_reset_grad)\n",
      "  %attention.out_proj.bias_grad.accumulation.out = InPlaceAccumulatorV2(%attention.out_proj.bias_grad.accumulation.buffer, %attention.out_proj.bias_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.weight_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.weight_grad.accumulation.buffer, %feed_forward.0.weight_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.bias_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.bias_grad.accumulation.buffer, %feed_forward.0.bias_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.weight_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.weight_grad.accumulation.buffer, %feed_forward.2.weight_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.bias_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.bias_grad.accumulation.buffer, %feed_forward.2.bias_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.weight_grad.accumulation.buffer, %layer_norm1.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.bias_grad.accumulation.buffer, %layer_norm1.bias_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.weight_grad.accumulation.buffer, %layer_norm2.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.bias_grad.accumulation.buffer, %layer_norm2.bias_grad, %lazy_reset_grad)\n",
      "  return %onnx::reducemean_output::6, %output, %attention.in_proj_weight_grad.accumulation.out, %attention.in_proj_bias_grad.accumulation.out, %attention.out_proj.weight_grad.accumulation.out, %attention.out_proj.bias_grad.accumulation.out, %feed_forward.0.weight_grad.accumulation.out, %feed_forward.0.bias_grad.accumulation.out, %feed_forward.2.weight_grad.accumulation.out, %feed_forward.2.bias_grad.accumulation.out, %layer_norm1.weight_grad.accumulation.out, %layer_norm1.bias_grad.accumulation.out, %layer_norm2.weight_grad.accumulation.out, %layer_norm2.bias_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"Transformer/training_model.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c03eff9-9b2c-412e-8e9b-b03632a997f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
