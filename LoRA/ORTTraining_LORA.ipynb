{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d7e84a-6c97-44f4-8525-1cf7ba04064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime.training import artifacts\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f2340-fabe-46f9-9af0-b61f1546f2ed",
   "metadata": {},
   "source": [
    "## LoRA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab347e4e-33a7-4d20-8c8c-8f8bba75c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        rank = 2\n",
    "        alpha = 4 \n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8053a73-89a5-4f8d-bcfd-1625cdfe3b74",
   "metadata": {},
   "source": [
    "## LoRA combined with the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce60db6-3237-43f7-851e-31bba1d0455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0db00-6e7b-468a-9cff-7896fd2919fd",
   "metadata": {},
   "source": [
    "### Single Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1817e8c-e2a9-4a25-8d15-08afbc8fbdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5490,  0.3671,  0.1219,  0.6466, -1.4168,  0.8429, -0.6307,  1.2340,\n",
      "          0.3127,  0.6972]])\n",
      "Linear(in_features=10, out_features=2, bias=True)\n",
      "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "random_seed=123\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "layer=nn.Linear(10,2)\n",
    "x=torch.randn((1, 10))\n",
    "\n",
    "print(x)\n",
    "print(layer)\n",
    "print('Original output:', layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57ac66-7a17-4671-bdb6-ab6c197d794d",
   "metadata": {},
   "source": [
    "### Applying LoRA to linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaea0c36-8de2-4ff3-965c-57a22e36372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_lora=LinearWithLoRA(layer)\n",
    "print(layer_lora(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fb03792-d0a5-4de0-aeee-7bc57ac6fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_lora=LinearWithLoRA(layer)\n",
    "model_outputs = layer_lora(x)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "185662aa-bb68-482d-8d7a-c820741c4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    layer_lora,\n",
    "    x,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff285de-dccd-4abb-9b6b-dd99cea69f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = [name for name, param in layer_lora.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in layer_lora.named_parameters() if not param.requires_grad]\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    #loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"Lora\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b7a249c-25a9-4076-afcd-8130dcc6a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex10]\n",
      "  %target[FLOAT, batch_sizex2]\n",
      "  %linear.weight[FLOAT, 2x10]\n",
      "  %linear.bias[FLOAT, 2]\n",
      "  %lora.A[FLOAT, 10x2]\n",
      "  %lora.B[FLOAT, 2x2]\n",
      "  %linear.weight_grad.accumulation.buffer[FLOAT, 2x10]\n",
      "  %linear.bias_grad.accumulation.buffer[FLOAT, 2]\n",
      "  %lora.A_grad.accumulation.buffer[FLOAT, 10x2]\n",
      "  %lora.B_grad.accumulation.buffer[FLOAT, 2x2]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %onnx::pow_exponent::3[FLOAT, 1]\n",
      "  %/lora/Constant_output_0[FLOAT, scalar]\n",
      "  %onnx::reducemean_output::6_grad[FLOAT, scalar]\n",
      "  %/linear/Gemm_Grad/ReduceAxes_for_/linear/Gemm_Grad/dC_reduced[INT64, 1]\n",
      "  %OneConstant_Type1[FLOAT, 1]\n",
      ") {\n",
      "  %/lora/MatMul_output_0 = MatMul(%input, %lora.A)\n",
      "  %/lora/MatMul_1_output_0 = MatMul(%/lora/MatMul_output_0, %lora.B)\n",
      "  %/lora/Mul_output_0 = Mul(%/lora/MatMul_1_output_0, %/lora/Constant_output_0)\n",
      "  %/linear/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%input, %linear.weight, %linear.bias)\n",
      "  %output = Add(%/linear/Gemm_output_0, %/lora/Mul_output_0)\n",
      "  %onnx::sub_output::1 = Sub(%output, %target)\n",
      "  %onnx::pow_output::4 = Pow(%onnx::sub_output::1, %onnx::pow_exponent::3)\n",
      "  %onnx::ReduceMean::7_Grad/Sized_X = Size(%onnx::pow_output::4)\n",
      "  %onnx::ReduceMean::7_Grad/Shaped_X = Shape(%onnx::pow_output::4)\n",
      "  %onnx::ReduceMean::7_Grad/Sized_Grad = Size(%onnx::reducemean_output::6_grad)\n",
      "  %onnx::ReduceMean::7_Grad/Scale = Div(%onnx::ReduceMean::7_Grad/Sized_X, %onnx::ReduceMean::7_Grad/Sized_Grad)\n",
      "  %onnx::ReduceMean::7_Grad/Scaled_Grad = Scale[scale_down = 1](%onnx::reducemean_output::6_grad, %onnx::ReduceMean::7_Grad/Scale)\n",
      "  %onnx::pow_output::4_grad = Expand(%onnx::ReduceMean::7_Grad/Scaled_Grad, %onnx::ReduceMean::7_Grad/Shaped_X)\n",
      "  %onnx::Pow::5_Grad/Sub_I1 = Sub(%onnx::pow_exponent::3, %OneConstant_Type1)\n",
      "  %onnx::Pow::5_Grad/Pow_I0 = Pow(%onnx::sub_output::1, %onnx::Pow::5_Grad/Sub_I1)\n",
      "  %onnx::Pow::5_Grad/Mul_Pow_I0_I1 = Mul(%onnx::Pow::5_Grad/Pow_I0, %onnx::pow_exponent::3)\n",
      "  %onnx::sub_output::1_grad = Mul(%onnx::Pow::5_Grad/Mul_Pow_I0_I1, %onnx::pow_output::4_grad)\n",
      "  %output_grad = Identity(%onnx::sub_output::1_grad)\n",
      "  %/lora/Mul_output_0_grad = Identity(%output_grad)\n",
      "  %/lora/Mul_Grad/PreReduceGrad0 = Mul(%/lora/Mul_output_0_grad, %/lora/Constant_output_0)\n",
      "  %/lora/MatMul_1_output_0_grad = Identity(%/lora/Mul_Grad/PreReduceGrad0)\n",
      "  %lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/lora/MatMul_output_0, %/lora/MatMul_1_output_0_grad)\n",
      "  %/lora/MatMul_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/lora/MatMul_1_output_0_grad, %lora.B)\n",
      "  %lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/lora/MatMul_output_0_grad)\n",
      "  %/linear/Gemm_output_0_grad = Identity(%output_grad)\n",
      "  %/linear/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/linear/Gemm_output_0_grad, %/linear/Gemm_Grad/ReduceAxes_for_/linear/Gemm_Grad/dC_reduced)\n",
      "  %linear.bias_grad = Identity(%/linear/Gemm_Grad/dC_reduced)\n",
      "  %linear.weight_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/linear/Gemm_output_0_grad, %input)\n",
      "  %onnx::reducemean_output::6 = ReduceMean[keepdims = 0](%onnx::pow_output::4)\n",
      "  %linear.weight_grad.accumulation.out = InPlaceAccumulatorV2(%linear.weight_grad.accumulation.buffer, %linear.weight_grad, %lazy_reset_grad)\n",
      "  %linear.bias_grad.accumulation.out = InPlaceAccumulatorV2(%linear.bias_grad.accumulation.buffer, %linear.bias_grad, %lazy_reset_grad)\n",
      "  %lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%lora.A_grad.accumulation.buffer, %lora.A_grad, %lazy_reset_grad)\n",
      "  %lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%lora.B_grad.accumulation.buffer, %lora.B_grad, %lazy_reset_grad)\n",
      "  return %onnx::reducemean_output::6, %output, %linear.weight_grad.accumulation.out, %linear.bias_grad.accumulation.out, %lora.A_grad.accumulation.out, %lora.B_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"Lora/training_model.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "300377f4-90a4-4f77-b8b3-e415d81798eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['linear.weight', 'linear.bias', 'lora.A', 'lora.B']\n"
     ]
    }
   ],
   "source": [
    "print(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff60ea12-240d-4e73-82ae-4949fcbe7740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(frozen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd74ed-4062-431a-9203-1181eec6f9be",
   "metadata": {},
   "source": [
    "## SimpleNet with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1af67a3e-69d9-4412-9637-6f163243ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch class that we will use to generate the graphs.\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, model_input):\n",
    "        out = self.fc1(model_input)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a63f2cf-b5ca-4c82-ba5f-6e0955ab1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance.\n",
    "device = \"cpu\"\n",
    "batch_size, input_size, hidden_size, output_size = 64, 784, 500, 10\n",
    "pt_model = Net(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a022111a-8cba-4db2-8fac-fdbdbaee9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        rank = 2\n",
    "        alpha = 4 \n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lora=self.lora.A @ self.lora.B # combine LoRA metrices\n",
    "        # then combine LoRA original weights\n",
    "        combined_weight=self.linear.weight+self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ae8097c-a229-464a-a3d3-aa8db003a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a86a071-c9e7-4343-a5e4-9777bca20e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_lora=copy.deepcopy(pt_model)\n",
    "print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "166e810c-e5c8-4c91-bf7d-bc2d22b4cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.fc1=LinearWithLoRAMerged(model_lora.fc1)\n",
    "model_lora.fc2=LinearWithLoRAMerged(model_lora.fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "094bc65c-7409-4355-94a3-239a9041e72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (fc2): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=500, out_features=10, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11e8311d-1fc3-4896-9936-355042340744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random input.\n",
    "model_inputs = (torch.randn(batch_size, input_size, device=device),)\n",
    "\n",
    "model_outputs = model_lora(*model_inputs)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    "\n",
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    model_lora,\n",
    "    model_inputs,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c70f5343-5232-44bc-a8a8-11d9ee6364c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = [name for name, param in model_lora.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in model_lora.named_parameters() if not param.requires_grad]\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    #loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"SimpleNet_Lora\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bb86b88-8a22-445b-b2e0-acdba2c620bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex784]\n",
      "  %target[FLOAT, batch_sizex10]\n",
      "  %fc1.linear.weight[FLOAT, 500x784]\n",
      "  %fc1.linear.bias[FLOAT, 500]\n",
      "  %fc1.lora.A[FLOAT, 784x2]\n",
      "  %fc1.lora.B[FLOAT, 2x500]\n",
      "  %fc2.linear.weight[FLOAT, 10x500]\n",
      "  %fc2.linear.bias[FLOAT, 10]\n",
      "  %fc2.lora.A[FLOAT, 500x2]\n",
      "  %fc2.lora.B[FLOAT, 2x10]\n",
      "  %fc1.linear.weight_grad.accumulation.buffer[FLOAT, 500x784]\n",
      "  %fc1.linear.bias_grad.accumulation.buffer[FLOAT, 500]\n",
      "  %fc1.lora.A_grad.accumulation.buffer[FLOAT, 784x2]\n",
      "  %fc1.lora.B_grad.accumulation.buffer[FLOAT, 2x500]\n",
      "  %fc2.linear.weight_grad.accumulation.buffer[FLOAT, 10x500]\n",
      "  %fc2.linear.bias_grad.accumulation.buffer[FLOAT, 10]\n",
      "  %fc2.lora.A_grad.accumulation.buffer[FLOAT, 500x2]\n",
      "  %fc2.lora.B_grad.accumulation.buffer[FLOAT, 2x10]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %onnx::pow_exponent::37[FLOAT, 1]\n",
      "  %/fc1/Constant_output_0[FLOAT, scalar]\n",
      "  %onnx::reducemean_output::40_grad[FLOAT, scalar]\n",
      "  %/fc1/Gemm_Grad/ReduceAxes_for_/fc1/Gemm_Grad/dC_reduced[INT64, 1]\n",
      "  %/fc2/Gemm_Grad/ReduceAxes_for_/fc2/Gemm_Grad/dC_reduced[INT64, 1]\n",
      "  %OneConstant_Type1[FLOAT, 1]\n",
      ") {\n",
      "  %/fc1/MatMul_output_0 = MatMul(%fc1.lora.A, %fc1.lora.B)\n",
      "  %/fc1/Transpose_output_0 = Transpose[perm = [1, 0]](%/fc1/MatMul_output_0)\n",
      "  %/fc1/Mul_output_0 = Mul(%/fc1/Transpose_output_0, %/fc1/Constant_output_0)\n",
      "  %/fc1/Add_output_0 = Add(%fc1.linear.weight, %/fc1/Mul_output_0)\n",
      "  %/fc1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%input, %/fc1/Add_output_0, %fc1.linear.bias)\n",
      "  %/relu/Relu_output_0 = Relu(%/fc1/Gemm_output_0)\n",
      "  %/fc2/MatMul_output_0 = MatMul(%fc2.lora.A, %fc2.lora.B)\n",
      "  %/fc2/Transpose_output_0 = Transpose[perm = [1, 0]](%/fc2/MatMul_output_0)\n",
      "  %/fc2/Mul_output_0 = Mul(%/fc2/Transpose_output_0, %/fc1/Constant_output_0)\n",
      "  %/fc2/Add_output_0 = Add(%fc2.linear.weight, %/fc2/Mul_output_0)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/relu/Relu_output_0, %/fc2/Add_output_0, %fc2.linear.bias)\n",
      "  %onnx::sub_output::35 = Sub(%output, %target)\n",
      "  %onnx::pow_output::38 = Pow(%onnx::sub_output::35, %onnx::pow_exponent::37)\n",
      "  %onnx::ReduceMean::41_Grad/Sized_X = Size(%onnx::pow_output::38)\n",
      "  %onnx::ReduceMean::41_Grad/Shaped_X = Shape(%onnx::pow_output::38)\n",
      "  %onnx::ReduceMean::41_Grad/Sized_Grad = Size(%onnx::reducemean_output::40_grad)\n",
      "  %onnx::ReduceMean::41_Grad/Scale = Div(%onnx::ReduceMean::41_Grad/Sized_X, %onnx::ReduceMean::41_Grad/Sized_Grad)\n",
      "  %onnx::ReduceMean::41_Grad/Scaled_Grad = Scale[scale_down = 1](%onnx::reducemean_output::40_grad, %onnx::ReduceMean::41_Grad/Scale)\n",
      "  %onnx::pow_output::38_grad = Expand(%onnx::ReduceMean::41_Grad/Scaled_Grad, %onnx::ReduceMean::41_Grad/Shaped_X)\n",
      "  %onnx::Pow::39_Grad/Sub_I1 = Sub(%onnx::pow_exponent::37, %OneConstant_Type1)\n",
      "  %onnx::Pow::39_Grad/Pow_I0 = Pow(%onnx::sub_output::35, %onnx::Pow::39_Grad/Sub_I1)\n",
      "  %onnx::Pow::39_Grad/Mul_Pow_I0_I1 = Mul(%onnx::Pow::39_Grad/Pow_I0, %onnx::pow_exponent::37)\n",
      "  %onnx::sub_output::35_grad = Mul(%onnx::Pow::39_Grad/Mul_Pow_I0_I1, %onnx::pow_output::38_grad)\n",
      "  %output_grad = Identity(%onnx::sub_output::35_grad)\n",
      "  %/fc2/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%output_grad, %/fc2/Gemm_Grad/ReduceAxes_for_/fc2/Gemm_Grad/dC_reduced)\n",
      "  %fc2.linear.bias_grad = Identity(%/fc2/Gemm_Grad/dC_reduced)\n",
      "  %/relu/Relu_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%output_grad, %/fc2/Add_output_0)\n",
      "  %/fc1/Gemm_output_0_grad = ReluGrad(%/relu/Relu_output_0_grad, %/relu/Relu_output_0)\n",
      "  %/fc1/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/fc1/Gemm_output_0_grad, %/fc1/Gemm_Grad/ReduceAxes_for_/fc1/Gemm_Grad/dC_reduced)\n",
      "  %fc1.linear.bias_grad = Identity(%/fc1/Gemm_Grad/dC_reduced)\n",
      "  %/fc1/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/fc1/Gemm_output_0_grad, %input)\n",
      "  %fc1.linear.weight_grad = Identity(%/fc1/Add_output_0_grad)\n",
      "  %/fc1/Mul_output_0_grad = Identity(%/fc1/Add_output_0_grad)\n",
      "  %/fc1/Mul_Grad/PreReduceGrad0 = Mul(%/fc1/Mul_output_0_grad, %/fc1/Constant_output_0)\n",
      "  %/fc1/Transpose_output_0_grad = Identity(%/fc1/Mul_Grad/PreReduceGrad0)\n",
      "  %/fc1/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/fc1/Transpose_output_0_grad)\n",
      "  %fc1.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%fc1.lora.A, %/fc1/MatMul_output_0_grad)\n",
      "  %fc1.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/fc1/MatMul_output_0_grad, %fc1.lora.B)\n",
      "  %/fc2/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%output_grad, %/relu/Relu_output_0)\n",
      "  %fc2.linear.weight_grad = Identity(%/fc2/Add_output_0_grad)\n",
      "  %/fc2/Mul_output_0_grad = Identity(%/fc2/Add_output_0_grad)\n",
      "  %/fc2/Mul_Grad/PreReduceGrad0 = Mul(%/fc2/Mul_output_0_grad, %/fc1/Constant_output_0)\n",
      "  %/fc2/Transpose_output_0_grad = Identity(%/fc2/Mul_Grad/PreReduceGrad0)\n",
      "  %/fc2/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/fc2/Transpose_output_0_grad)\n",
      "  %fc2.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%fc2.lora.A, %/fc2/MatMul_output_0_grad)\n",
      "  %fc2.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/fc2/MatMul_output_0_grad, %fc2.lora.B)\n",
      "  %onnx::reducemean_output::40 = ReduceMean[keepdims = 0](%onnx::pow_output::38)\n",
      "  %fc1.linear.weight_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.linear.weight_grad.accumulation.buffer, %fc1.linear.weight_grad, %lazy_reset_grad)\n",
      "  %fc1.linear.bias_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.linear.bias_grad.accumulation.buffer, %fc1.linear.bias_grad, %lazy_reset_grad)\n",
      "  %fc1.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.lora.A_grad.accumulation.buffer, %fc1.lora.A_grad, %lazy_reset_grad)\n",
      "  %fc1.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.lora.B_grad.accumulation.buffer, %fc1.lora.B_grad, %lazy_reset_grad)\n",
      "  %fc2.linear.weight_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.linear.weight_grad.accumulation.buffer, %fc2.linear.weight_grad, %lazy_reset_grad)\n",
      "  %fc2.linear.bias_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.linear.bias_grad.accumulation.buffer, %fc2.linear.bias_grad, %lazy_reset_grad)\n",
      "  %fc2.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.lora.A_grad.accumulation.buffer, %fc2.lora.A_grad, %lazy_reset_grad)\n",
      "  %fc2.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.lora.B_grad.accumulation.buffer, %fc2.lora.B_grad, %lazy_reset_grad)\n",
      "  return %onnx::reducemean_output::40, %output, %fc1.linear.weight_grad.accumulation.out, %fc1.linear.bias_grad.accumulation.out, %fc1.lora.A_grad.accumulation.out, %fc1.lora.B_grad.accumulation.out, %fc2.linear.weight_grad.accumulation.out, %fc2.linear.bias_grad.accumulation.out, %fc2.lora.A_grad.accumulation.out, %fc2.lora.B_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"SimpleNET_Lora/training_model.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73e60ff6-8d40-4f3a-b3c3-5ffb0260bafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fc1.linear.weight', 'fc1.linear.bias', 'fc1.lora.A', 'fc1.lora.B', 'fc2.linear.weight', 'fc2.linear.bias', 'fc2.lora.A', 'fc2.lora.B']\n"
     ]
    }
   ],
   "source": [
    "print(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10e72fd7-c546-4a64-b17a-bc75ceac6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(frozen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33e4a4-3bd8-4bd4-a6b1-8234e6ef638e",
   "metadata": {},
   "source": [
    "## Activating only the LoRA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e200295-1c0b-4cd4-9310-edb97b41b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.linear.weight:False\n",
      "fc1.linear.bias:False\n",
      "fc1.lora.A:True\n",
      "fc1.lora.B:True\n",
      "fc2.linear.weight:False\n",
      "fc2.linear.bias:False\n",
      "fc2.lora.A:True\n",
      "fc2.lora.B:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6802c4d-7152-406e-bbdf-ae3eb52eac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = [name for name, param in model_lora.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in model_lora.named_parameters() if not param.requires_grad]\n",
    "\n",
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    #loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"SimpleNet_Lora\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfe8615f-1172-405c-85ff-c6cd912caa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fc1.lora.A', 'fc1.lora.B', 'fc2.lora.A', 'fc2.lora.B']\n"
     ]
    }
   ],
   "source": [
    "print(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47efafd8-90cd-4ab5-b3f0-4e3693c27847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex784]\n",
      "  %target[FLOAT, batch_sizex10]\n",
      "  %fc1.linear.weight[FLOAT, 500x784]\n",
      "  %fc1.linear.bias[FLOAT, 500]\n",
      "  %fc1.lora.A[FLOAT, 784x2]\n",
      "  %fc1.lora.B[FLOAT, 2x500]\n",
      "  %fc2.linear.weight[FLOAT, 10x500]\n",
      "  %fc2.linear.bias[FLOAT, 10]\n",
      "  %fc2.lora.A[FLOAT, 500x2]\n",
      "  %fc2.lora.B[FLOAT, 2x10]\n",
      "  %fc1.lora.A_grad.accumulation.buffer[FLOAT, 784x2]\n",
      "  %fc1.lora.B_grad.accumulation.buffer[FLOAT, 2x500]\n",
      "  %fc2.lora.A_grad.accumulation.buffer[FLOAT, 500x2]\n",
      "  %fc2.lora.B_grad.accumulation.buffer[FLOAT, 2x10]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %onnx::pow_exponent::46[FLOAT, 1]\n",
      "  %/fc1/Constant_output_0[FLOAT, scalar]\n",
      "  %onnx::reducemean_output::49_grad[FLOAT, scalar]\n",
      "  %OneConstant_Type1[FLOAT, 1]\n",
      ") {\n",
      "  %/fc1/MatMul_output_0 = MatMul(%fc1.lora.A, %fc1.lora.B)\n",
      "  %/fc1/Transpose_output_0 = Transpose[perm = [1, 0]](%/fc1/MatMul_output_0)\n",
      "  %/fc1/Mul_output_0 = Mul(%/fc1/Transpose_output_0, %/fc1/Constant_output_0)\n",
      "  %/fc1/Add_output_0 = Add(%fc1.linear.weight, %/fc1/Mul_output_0)\n",
      "  %/fc1/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%input, %/fc1/Add_output_0, %fc1.linear.bias)\n",
      "  %/relu/Relu_output_0 = Relu(%/fc1/Gemm_output_0)\n",
      "  %/fc2/MatMul_output_0 = MatMul(%fc2.lora.A, %fc2.lora.B)\n",
      "  %/fc2/Transpose_output_0 = Transpose[perm = [1, 0]](%/fc2/MatMul_output_0)\n",
      "  %/fc2/Mul_output_0 = Mul(%/fc2/Transpose_output_0, %/fc1/Constant_output_0)\n",
      "  %/fc2/Add_output_0 = Add(%fc2.linear.weight, %/fc2/Mul_output_0)\n",
      "  %output = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/relu/Relu_output_0, %/fc2/Add_output_0, %fc2.linear.bias)\n",
      "  %onnx::sub_output::44 = Sub(%output, %target)\n",
      "  %onnx::pow_output::47 = Pow(%onnx::sub_output::44, %onnx::pow_exponent::46)\n",
      "  %onnx::ReduceMean::50_Grad/Sized_X = Size(%onnx::pow_output::47)\n",
      "  %onnx::ReduceMean::50_Grad/Shaped_X = Shape(%onnx::pow_output::47)\n",
      "  %onnx::ReduceMean::50_Grad/Sized_Grad = Size(%onnx::reducemean_output::49_grad)\n",
      "  %onnx::ReduceMean::50_Grad/Scale = Div(%onnx::ReduceMean::50_Grad/Sized_X, %onnx::ReduceMean::50_Grad/Sized_Grad)\n",
      "  %onnx::ReduceMean::50_Grad/Scaled_Grad = Scale[scale_down = 1](%onnx::reducemean_output::49_grad, %onnx::ReduceMean::50_Grad/Scale)\n",
      "  %onnx::pow_output::47_grad = Expand(%onnx::ReduceMean::50_Grad/Scaled_Grad, %onnx::ReduceMean::50_Grad/Shaped_X)\n",
      "  %onnx::Pow::48_Grad/Sub_I1 = Sub(%onnx::pow_exponent::46, %OneConstant_Type1)\n",
      "  %onnx::Pow::48_Grad/Pow_I0 = Pow(%onnx::sub_output::44, %onnx::Pow::48_Grad/Sub_I1)\n",
      "  %onnx::Pow::48_Grad/Mul_Pow_I0_I1 = Mul(%onnx::Pow::48_Grad/Pow_I0, %onnx::pow_exponent::46)\n",
      "  %onnx::sub_output::44_grad = Mul(%onnx::Pow::48_Grad/Mul_Pow_I0_I1, %onnx::pow_output::47_grad)\n",
      "  %output_grad = Identity(%onnx::sub_output::44_grad)\n",
      "  %/relu/Relu_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%output_grad, %/fc2/Add_output_0)\n",
      "  %/fc1/Gemm_output_0_grad = ReluGrad(%/relu/Relu_output_0_grad, %/relu/Relu_output_0)\n",
      "  %/fc1/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/fc1/Gemm_output_0_grad, %input)\n",
      "  %/fc1/Mul_output_0_grad = Identity(%/fc1/Add_output_0_grad)\n",
      "  %/fc1/Mul_Grad/PreReduceGrad0 = Mul(%/fc1/Mul_output_0_grad, %/fc1/Constant_output_0)\n",
      "  %/fc1/Transpose_output_0_grad = Identity(%/fc1/Mul_Grad/PreReduceGrad0)\n",
      "  %/fc1/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/fc1/Transpose_output_0_grad)\n",
      "  %fc1.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%fc1.lora.A, %/fc1/MatMul_output_0_grad)\n",
      "  %fc1.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/fc1/MatMul_output_0_grad, %fc1.lora.B)\n",
      "  %/fc2/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%output_grad, %/relu/Relu_output_0)\n",
      "  %/fc2/Mul_output_0_grad = Identity(%/fc2/Add_output_0_grad)\n",
      "  %/fc2/Mul_Grad/PreReduceGrad0 = Mul(%/fc2/Mul_output_0_grad, %/fc1/Constant_output_0)\n",
      "  %/fc2/Transpose_output_0_grad = Identity(%/fc2/Mul_Grad/PreReduceGrad0)\n",
      "  %/fc2/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/fc2/Transpose_output_0_grad)\n",
      "  %fc2.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%fc2.lora.A, %/fc2/MatMul_output_0_grad)\n",
      "  %fc2.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/fc2/MatMul_output_0_grad, %fc2.lora.B)\n",
      "  %onnx::reducemean_output::49 = ReduceMean[keepdims = 0](%onnx::pow_output::47)\n",
      "  %fc1.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.lora.A_grad.accumulation.buffer, %fc1.lora.A_grad, %lazy_reset_grad)\n",
      "  %fc1.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%fc1.lora.B_grad.accumulation.buffer, %fc1.lora.B_grad, %lazy_reset_grad)\n",
      "  %fc2.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.lora.A_grad.accumulation.buffer, %fc2.lora.A_grad, %lazy_reset_grad)\n",
      "  %fc2.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%fc2.lora.B_grad.accumulation.buffer, %fc2.lora.B_grad, %lazy_reset_grad)\n",
      "  return %onnx::reducemean_output::49, %output, %fc1.lora.A_grad.accumulation.out, %fc1.lora.B_grad.accumulation.out, %fc2.lora.A_grad.accumulation.out, %fc2.lora.B_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"SimpleNET_Lora/training_model.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108d4c9-883c-4f94-8787-50a639a2c206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
