{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83a5de2-4ab3-415c-b028-6ba66e9b95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime.training.onnxblock as onnxblock\n",
    "from onnxruntime.training.api import CheckpointState, Module, Optimizer\n",
    "from onnxruntime.training import artifacts\n",
    "from onnxruntime import InferenceSession\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef68de-84dc-44e0-832e-bb0b9cc402d4",
   "metadata": {},
   "source": [
    "## Create the Transformer (Attention + FFN) block : declare Q,K,V as linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7aac2b-26dc-4127-9529-d947f705eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads,dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        d_in = 32\n",
    "        d_out =32\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out,bias=False)\n",
    "        self.W_key   = nn.Linear(d_in, d_out,bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out,bias=False)\n",
    "        #self.W_out = nn.Linear(d_in,d_out,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        #self.attention = nn.MultiheadAttention(embed_size, heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head self-attention\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        #out = self.W_out(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T # Changed transpose\n",
    "        #attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "        #    self.mask.bool()[:n_tokens, :n_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        attn_output = attn_weights @ values\n",
    "        #return context_vec\n",
    "\n",
    "        # Add and norm\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward network\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        ff_output = self.dropout(ff_output)\n",
    "        # Add and norm\n",
    "        out2 = self.layer_norm2(out1 + ff_output)\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6c55d6-b481-4a61-bf08-8a3d0de24088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 32])\n",
      "TransformerBlock(\n",
      "  (W_query): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_key): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_value): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example usage of TransformerBlock\n",
    "#input_tensor = torch.randn(10, 32, 512)  # (sequence_length, batch_size, embed_size).\n",
    "#input_tensor = torch.randn(10, 32, 32) \n",
    "inputs = torch.rand(6,32)\n",
    "transformer_block = TransformerBlock(embed_size=32, heads=8)\n",
    "output_tensor = transformer_block(inputs)\n",
    "print(output_tensor.shape)  # should print: torch.Size([10, 32, 512])\n",
    "print(transformer_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d61d7-f369-4ebc-8d17-83c5f5b4487b",
   "metadata": {},
   "source": [
    "## Lora Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5221dc73-f28a-41db-91a8-f9d3b77cc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        rank = 2\n",
    "        alpha = 4 \n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f92bf2-f6d3-4400-bbb3-7c9a4988557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5683b46f-8821-4e66-a7f1-41f31360304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        rank = 2\n",
    "        alpha = 4 \n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lora=self.lora.A @ self.lora.B # combine LoRA metrices\n",
    "        # then combine LoRA original weights\n",
    "        combined_weight=self.linear.weight+self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbbfda-e087-461e-bacb-7a244c5a95c4",
   "metadata": {},
   "source": [
    "## Add Lora layers to the transformer : Q & V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c8ee6c-1092-45ca-8fb3-0977a59320b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a1bb75-ec5f-4acd-9255-477499356331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (W_query): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_key): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_value): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer_lora = copy.deepcopy(transformer_block)\n",
    "print(transformer_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840d87dc-2acc-45aa-9adf-4eabee120aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lora.W_query=LinearWithLoRAMerged(transformer_lora.W_query)\n",
    "transformer_lora.W_value=LinearWithLoRAMerged(transformer_lora.W_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f8c233-1267-4296-8818-b08b38bb0f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (W_query): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (W_key): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_value): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a53405b-bca3-47d7-9b11-3689ea4dce01",
   "metadata": {},
   "source": [
    "## Create ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a5229f-025f-43b9-b0d1-0538421fa393",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = transformer_lora(inputs)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466d6c74-6011-408b-9003-5d907a392555",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    transformer_lora,\n",
    "    inputs,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a88779f-42a1-4f7f-9ac3-745e21b2caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad = [name for name, param in transformer_lora.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in transformer_lora.named_parameters() if not param.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ba3350-a03b-48fc-98e1-586913b31a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W_query.linear.weight', 'W_query.lora.A', 'W_query.lora.B', 'W_key.weight', 'W_value.linear.weight', 'W_value.lora.A', 'W_value.lora.B', 'feed_forward.0.weight', 'feed_forward.0.bias', 'feed_forward.2.weight', 'feed_forward.2.bias', 'layer_norm1.weight', 'layer_norm1.bias', 'layer_norm2.weight', 'layer_norm2.bias']\n"
     ]
    }
   ],
   "source": [
    "print(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed9b764b-e5bf-4daa-a81d-5e93980dd54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.linear.weight:False\n",
      "W_query.lora.A:True\n",
      "W_query.lora.B:True\n",
      "W_key.weight:False\n",
      "W_value.linear.weight:False\n",
      "W_value.lora.A:True\n",
      "W_value.lora.B:True\n",
      "feed_forward.0.weight:False\n",
      "feed_forward.0.bias:False\n",
      "feed_forward.2.weight:False\n",
      "feed_forward.2.bias:False\n",
      "layer_norm1.weight:True\n",
      "layer_norm1.bias:True\n",
      "layer_norm2.weight:True\n",
      "layer_norm2.bias:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(transformer_lora)\n",
    "for name, param in transformer_lora.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb25ce5-4642-42f7-9d1d-83efa6882916",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    #loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"Transformer_Lora\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc4edf6-172f-45b4-a2a8-b6e27f2ef0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex32]\n",
      "  %labels[INT64, batch_size]\n",
      "  %W_query.linear.weight[FLOAT, 32x32]\n",
      "  %W_query.lora.A[FLOAT, 32x2]\n",
      "  %W_query.lora.B[FLOAT, 2x32]\n",
      "  %W_key.weight[FLOAT, 32x32]\n",
      "  %W_value.linear.weight[FLOAT, 32x32]\n",
      "  %W_value.lora.A[FLOAT, 32x2]\n",
      "  %W_value.lora.B[FLOAT, 2x32]\n",
      "  %feed_forward.0.weight[FLOAT, 128x32]\n",
      "  %feed_forward.0.bias[FLOAT, 128]\n",
      "  %feed_forward.2.weight[FLOAT, 32x128]\n",
      "  %feed_forward.2.bias[FLOAT, 32]\n",
      "  %layer_norm1.weight[FLOAT, 32]\n",
      "  %layer_norm1.bias[FLOAT, 32]\n",
      "  %layer_norm2.weight[FLOAT, 32]\n",
      "  %layer_norm2.bias[FLOAT, 32]\n",
      "  %W_query.linear.weight_grad.accumulation.buffer[FLOAT, 32x32]\n",
      "  %W_query.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %W_query.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %W_key.weight_grad.accumulation.buffer[FLOAT, 32x32]\n",
      "  %W_value.linear.weight_grad.accumulation.buffer[FLOAT, 32x32]\n",
      "  %W_value.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %W_value.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %feed_forward.0.weight_grad.accumulation.buffer[FLOAT, 128x32]\n",
      "  %feed_forward.0.bias_grad.accumulation.buffer[FLOAT, 128]\n",
      "  %feed_forward.2.weight_grad.accumulation.buffer[FLOAT, 32x128]\n",
      "  %feed_forward.2.bias_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm1.weight_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm1.bias_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm2.weight_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm2.bias_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %/W_query/Constant_output_0[FLOAT, scalar]\n",
      "  %/dropout_1/Constant_1_output_0[BOOL, scalar]\n",
      "  %/dropout/Constant_1_output_0[BOOL, scalar]\n",
      "  %/Pow_output_0[FLOAT, scalar]\n",
      "  %/dropout/Constant_output_0[FLOAT, scalar]\n",
      "  %onnx::loss::2_grad[FLOAT, scalar]\n",
      "  %/feed_forward/feed_forward.0/Gemm_Grad/ReduceAxes_for_/feed_forward/feed_forward.0/Gemm_Grad/dC_reduced[INT64, 1]\n",
      "  %/feed_forward/feed_forward.2/Gemm_Grad/ReduceAxes_for_/feed_forward/feed_forward.2/Gemm_Grad/dC_reduced[INT64, 1]\n",
      ") {\n",
      "  %/W_value/MatMul_output_0 = MatMul(%W_value.lora.A, %W_value.lora.B)\n",
      "  %/W_value/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_value/MatMul_output_0)\n",
      "  %/W_value/Mul_output_0 = Mul(%/W_value/Transpose_output_0, %/W_query/Constant_output_0)\n",
      "  %/W_value/Add_output_0 = Add(%W_value.linear.weight, %/W_value/Mul_output_0)\n",
      "  %/W_value/Transpose_1_output_0 = Transpose[perm = [1, 0]](%/W_value/Add_output_0)\n",
      "  %/W_value/MatMul_1_output_0 = MatMul(%input, %/W_value/Transpose_1_output_0)\n",
      "  %/W_query/MatMul_output_0 = MatMul(%W_query.lora.A, %W_query.lora.B)\n",
      "  %/W_query/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_query/MatMul_output_0)\n",
      "  %/W_query/Mul_output_0 = Mul(%/W_query/Transpose_output_0, %/W_query/Constant_output_0)\n",
      "  %/W_query/Add_output_0 = Add(%W_query.linear.weight, %/W_query/Mul_output_0)\n",
      "  %/W_query/Transpose_1_output_0 = Transpose[perm = [1, 0]](%/W_query/Add_output_0)\n",
      "  %/W_query/MatMul_1_output_0 = MatMul(%input, %/W_query/Transpose_1_output_0)\n",
      "  %/W_key/Transpose_output_0 = Transpose[perm = [1, 0]](%W_key.weight)\n",
      "  %/W_key/MatMul_output_0 = MatMul(%input, %/W_key/Transpose_output_0)\n",
      "  %/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_key/MatMul_output_0)\n",
      "  %/MatMul_output_0 = MatMul(%/W_query/MatMul_1_output_0, %/Transpose_output_0)\n",
      "  %/Div_output_0 = Div(%/MatMul_output_0, %/Pow_output_0)\n",
      "  %/Softmax_output_0 = Softmax[axis = -1](%/Div_output_0)\n",
      "  %/dropout/Dropout_output_0, %/dropout/Dropout_output_1 = Dropout(%/Softmax_output_0, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/MatMul_1_output_0 = MatMul(%/dropout/Dropout_output_0, %/W_value/MatMul_1_output_0)\n",
      "  %/Add_output_0 = Add(%input, %/MatMul_1_output_0)\n",
      "  %/layer_norm1/Add_1_output_0, %saved_mean, %saved_inv_std_var = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_output_0, %layer_norm1.weight, %layer_norm1.bias)\n",
      "  %/feed_forward/feed_forward.0/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/layer_norm1/Add_1_output_0, %feed_forward.0.weight, %feed_forward.0.bias)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0 = Relu(%/feed_forward/feed_forward.0/Gemm_output_0)\n",
      "  %/feed_forward/feed_forward.2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/feed_forward/feed_forward.1/Relu_output_0, %feed_forward.2.weight, %feed_forward.2.bias)\n",
      "  %/dropout_1/Dropout_output_0, %/dropout_1/Dropout_output_1 = Dropout(%/feed_forward/feed_forward.2/Gemm_output_0, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %/Add_1_output_0 = Add(%/layer_norm1/Add_1_output_0, %/dropout_1/Dropout_output_0)\n",
      "  %output, %saved_mean_token_0, %saved_inv_std_var_token_1 = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_1_output_0, %layer_norm2.weight, %layer_norm2.bias)\n",
      "  %onnx::loss::2, %onnx::log_prob::3 = SoftmaxCrossEntropyLoss[reduction = 'mean'](%output, %labels)\n",
      "  %output_grad = SoftmaxCrossEntropyLossGrad[reduction = 'mean'](%onnx::loss::2_grad, %onnx::log_prob::3, %labels)\n",
      "  %/Add_1_output_0_grad, %layer_norm2.weight_grad, %layer_norm2.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%output_grad, %/Add_1_output_0, %layer_norm2.weight, %saved_mean_token_0, %saved_inv_std_var_token_1)\n",
      "  %/dropout_1/Dropout_output_0_grad = Identity(%/Add_1_output_0_grad)\n",
      "  %/feed_forward/feed_forward.2/Gemm_output_0_grad = DropoutGrad(%/dropout_1/Dropout_output_0_grad, %/dropout_1/Dropout_output_1, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %/feed_forward/feed_forward.2/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/feed_forward/feed_forward.2/Gemm_output_0_grad, %/feed_forward/feed_forward.2/Gemm_Grad/ReduceAxes_for_/feed_forward/feed_forward.2/Gemm_Grad/dC_reduced)\n",
      "  %feed_forward.2.bias_grad = Identity(%/feed_forward/feed_forward.2/Gemm_Grad/dC_reduced)\n",
      "  %feed_forward.2.weight_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/feed_forward/feed_forward.2/Gemm_output_0_grad, %/feed_forward/feed_forward.1/Relu_output_0)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%/feed_forward/feed_forward.2/Gemm_output_0_grad, %feed_forward.2.weight)\n",
      "  %/feed_forward/feed_forward.0/Gemm_output_0_grad = ReluGrad(%/feed_forward/feed_forward.1/Relu_output_0_grad, %/feed_forward/feed_forward.1/Relu_output_0)\n",
      "  %/feed_forward/feed_forward.0/Gemm_Grad/dC_reduced = ReduceSum[keepdims = 0, noop_with_empty_axes = 0](%/feed_forward/feed_forward.0/Gemm_output_0_grad, %/feed_forward/feed_forward.0/Gemm_Grad/ReduceAxes_for_/feed_forward/feed_forward.0/Gemm_Grad/dC_reduced)\n",
      "  %feed_forward.0.bias_grad = Identity(%/feed_forward/feed_forward.0/Gemm_Grad/dC_reduced)\n",
      "  %feed_forward.0.weight_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/feed_forward/feed_forward.0/Gemm_output_0_grad, %/layer_norm1/Add_1_output_0)\n",
      "  %/layer_norm1/Add_1_output_0_grad_1 = Identity(%/Add_1_output_0_grad)\n",
      "  %/layer_norm1/Add_1_output_0_grad_0 = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%/feed_forward/feed_forward.0/Gemm_output_0_grad, %feed_forward.0.weight)\n",
      "  %/layer_norm1/Add_1_output_0_grad = Sum(%/layer_norm1/Add_1_output_0_grad_0, %/layer_norm1/Add_1_output_0_grad_1)\n",
      "  %/Add_output_0_grad, %layer_norm1.weight_grad, %layer_norm1.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/layer_norm1/Add_1_output_0_grad, %/Add_output_0, %layer_norm1.weight, %saved_mean, %saved_inv_std_var)\n",
      "  %/MatMul_1_output_0_grad = Identity(%/Add_output_0_grad)\n",
      "  %/W_value/MatMul_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/dropout/Dropout_output_0, %/MatMul_1_output_0_grad)\n",
      "  %/W_value/Transpose_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_value/MatMul_1_output_0_grad)\n",
      "  %/W_value/Add_output_0_grad = Transpose[perm = [1, 0]](%/W_value/Transpose_1_output_0_grad)\n",
      "  %W_value.linear.weight_grad = Identity(%/W_value/Add_output_0_grad)\n",
      "  %/W_value/Mul_output_0_grad = Identity(%/W_value/Add_output_0_grad)\n",
      "  %/W_value/Mul_Grad/PreReduceGrad0 = Mul(%/W_value/Mul_output_0_grad, %/W_query/Constant_output_0)\n",
      "  %/W_value/Transpose_output_0_grad = Identity(%/W_value/Mul_Grad/PreReduceGrad0)\n",
      "  %/W_value/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/W_value/Transpose_output_0_grad)\n",
      "  %W_value.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%W_value.lora.A, %/W_value/MatMul_output_0_grad)\n",
      "  %W_value.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/W_value/MatMul_output_0_grad, %W_value.lora.B)\n",
      "  %/dropout/Dropout_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/MatMul_1_output_0_grad, %/W_value/MatMul_1_output_0)\n",
      "  %/Softmax_output_0_grad = DropoutGrad(%/dropout/Dropout_output_0_grad, %/dropout/Dropout_output_1, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/Div_output_0_grad = SoftmaxGrad_13[axis = -1](%/Softmax_output_0_grad, %/Softmax_output_0)\n",
      "  %/Div_Grad/PreReduceGrad0 = Div(%/Div_output_0_grad, %/Pow_output_0)\n",
      "  %/MatMul_output_0_grad = Identity(%/Div_Grad/PreReduceGrad0)\n",
      "  %/W_query/MatMul_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/MatMul_output_0_grad, %/Transpose_output_0)\n",
      "  %/W_query/Transpose_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_query/MatMul_1_output_0_grad)\n",
      "  %/W_query/Add_output_0_grad = Transpose[perm = [1, 0]](%/W_query/Transpose_1_output_0_grad)\n",
      "  %W_query.linear.weight_grad = Identity(%/W_query/Add_output_0_grad)\n",
      "  %/W_query/Mul_output_0_grad = Identity(%/W_query/Add_output_0_grad)\n",
      "  %/W_query/Mul_Grad/PreReduceGrad0 = Mul(%/W_query/Mul_output_0_grad, %/W_query/Constant_output_0)\n",
      "  %/W_query/Transpose_output_0_grad = Identity(%/W_query/Mul_Grad/PreReduceGrad0)\n",
      "  %/W_query/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/W_query/Transpose_output_0_grad)\n",
      "  %W_query.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%W_query.lora.A, %/W_query/MatMul_output_0_grad)\n",
      "  %W_query.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/W_query/MatMul_output_0_grad, %W_query.lora.B)\n",
      "  %/Transpose_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/W_query/MatMul_1_output_0, %/MatMul_output_0_grad)\n",
      "  %/W_key/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/Transpose_output_0_grad)\n",
      "  %/W_key/Transpose_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_key/MatMul_output_0_grad)\n",
      "  %W_key.weight_grad = Transpose[perm = [1, 0]](%/W_key/Transpose_output_0_grad)\n",
      "  %W_query.linear.weight_grad.accumulation.out = InPlaceAccumulatorV2(%W_query.linear.weight_grad.accumulation.buffer, %W_query.linear.weight_grad, %lazy_reset_grad)\n",
      "  %W_query.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%W_query.lora.A_grad.accumulation.buffer, %W_query.lora.A_grad, %lazy_reset_grad)\n",
      "  %W_query.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%W_query.lora.B_grad.accumulation.buffer, %W_query.lora.B_grad, %lazy_reset_grad)\n",
      "  %W_key.weight_grad.accumulation.out = InPlaceAccumulatorV2(%W_key.weight_grad.accumulation.buffer, %W_key.weight_grad, %lazy_reset_grad)\n",
      "  %W_value.linear.weight_grad.accumulation.out = InPlaceAccumulatorV2(%W_value.linear.weight_grad.accumulation.buffer, %W_value.linear.weight_grad, %lazy_reset_grad)\n",
      "  %W_value.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%W_value.lora.A_grad.accumulation.buffer, %W_value.lora.A_grad, %lazy_reset_grad)\n",
      "  %W_value.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%W_value.lora.B_grad.accumulation.buffer, %W_value.lora.B_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.weight_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.weight_grad.accumulation.buffer, %feed_forward.0.weight_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.bias_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.bias_grad.accumulation.buffer, %feed_forward.0.bias_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.weight_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.weight_grad.accumulation.buffer, %feed_forward.2.weight_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.bias_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.bias_grad.accumulation.buffer, %feed_forward.2.bias_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.weight_grad.accumulation.buffer, %layer_norm1.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.bias_grad.accumulation.buffer, %layer_norm1.bias_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.weight_grad.accumulation.buffer, %layer_norm2.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.bias_grad.accumulation.buffer, %layer_norm2.bias_grad, %lazy_reset_grad)\n",
      "  return %onnx::loss::2, %output, %W_query.linear.weight_grad.accumulation.out, %W_query.lora.A_grad.accumulation.out, %W_query.lora.B_grad.accumulation.out, %W_key.weight_grad.accumulation.out, %W_value.linear.weight_grad.accumulation.out, %W_value.lora.A_grad.accumulation.out, %W_value.lora.B_grad.accumulation.out, %feed_forward.0.weight_grad.accumulation.out, %feed_forward.0.bias_grad.accumulation.out, %feed_forward.2.weight_grad.accumulation.out, %feed_forward.2.bias_grad.accumulation.out, %layer_norm1.weight_grad.accumulation.out, %layer_norm1.bias_grad.accumulation.out, %layer_norm2.weight_grad.accumulation.out, %layer_norm2.bias_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"Transformer_Lora/training_model.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae6ad8-fc1d-4e03-a2d2-4d6b0914b734",
   "metadata": {},
   "source": [
    "## To add LoRA to all layers \n",
    "#### Accessing the FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6825cc5-7fb6-4b38-b848-9f3f095c5fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lora.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f809e1d-6172-4beb-afaa-1967360b56cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=32, out_features=128, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=128, out_features=32, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lora.feed_forward[0])\n",
    "print(transformer_lora.feed_forward[1])\n",
    "print(transformer_lora.feed_forward[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912f5726-a676-4747-acc7-a34f96c0c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (W_query): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (W_key): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (W_value): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer_lora.W_key=LinearWithLoRAMerged(transformer_lora.W_key)\n",
    "transformer_lora.feed_forward[0]=LinearWithLoRAMerged(transformer_lora.feed_forward[0])\n",
    "transformer_lora.feed_forward[2]=LinearWithLoRAMerged(transformer_lora.feed_forward[2])\n",
    "print(transformer_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "664cdfbd-fbb3-4198-8419-2fe73d1f8c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.linear.weight:False\n",
      "W_query.lora.A:True\n",
      "W_query.lora.B:True\n",
      "W_key.linear.weight:False\n",
      "W_key.lora.A:True\n",
      "W_key.lora.B:True\n",
      "W_value.linear.weight:False\n",
      "W_value.lora.A:True\n",
      "W_value.lora.B:True\n",
      "feed_forward.0.linear.weight:False\n",
      "feed_forward.0.linear.bias:False\n",
      "feed_forward.0.lora.A:True\n",
      "feed_forward.0.lora.B:True\n",
      "feed_forward.2.linear.weight:False\n",
      "feed_forward.2.linear.bias:False\n",
      "feed_forward.2.lora.A:True\n",
      "feed_forward.2.lora.B:True\n",
      "layer_norm1.weight:True\n",
      "layer_norm1.bias:True\n",
      "layer_norm2.weight:True\n",
      "layer_norm2.bias:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(transformer_lora)\n",
    "for name, param in transformer_lora.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32247a0f-6cf2-4976-beb1-6d56e2f3d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = transformer_lora(inputs)\n",
    "if isinstance(model_outputs, torch.Tensor):\n",
    "    model_outputs = [model_outputs]\n",
    "    \n",
    "input_names = [\"input\"]\n",
    "output_names = [\"output\"]\n",
    "dynamic_axes = {\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b597666f-e6fd-4106-b1bb-39641ef6ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.BytesIO()\n",
    "torch.onnx.export(\n",
    "    transformer_lora,\n",
    "    inputs,\n",
    "    f,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=False,\n",
    "    training=torch.onnx.TrainingMode.TRAINING,\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    export_params=True,\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f612149d-a7fe-482e-90ec-79d1287c8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W_query.lora.A', 'W_query.lora.B', 'W_key.lora.A', 'W_key.lora.B', 'W_value.lora.A', 'W_value.lora.B', 'feed_forward.0.lora.A', 'feed_forward.0.lora.B', 'feed_forward.2.lora.A', 'feed_forward.2.lora.B', 'layer_norm1.weight', 'layer_norm1.bias', 'layer_norm2.weight', 'layer_norm2.bias']\n"
     ]
    }
   ],
   "source": [
    "requires_grad = [name for name, param in transformer_lora.named_parameters() if param.requires_grad]\n",
    "\n",
    "frozen_params = [name for name, param in transformer_lora.named_parameters() if not param.requires_grad]\n",
    "print(requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb560d74-cf83-4661-81e5-dd66de33d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts.generate_artifacts(\n",
    "    onnx_model,\n",
    "    optimizer=artifacts.OptimType.AdamW,\n",
    "    loss=artifacts.LossType.CrossEntropyLoss, #Specify the loss function, try with different ones\n",
    "    #loss=artifacts.LossType.MSELoss,\n",
    "    requires_grad=requires_grad,\n",
    "    frozen_params=frozen_params,\n",
    "    artifact_directory=\"Transformer_Lora/All_layers\",\n",
    "    additional_output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eacb37d-6512-4d3b-addd-f3fc02a9a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :\n",
      "\n",
      "graph main_graph (\n",
      "  %input[FLOAT, batch_sizex32]\n",
      "  %labels[INT64, batch_size]\n",
      "  %W_query.linear.weight[FLOAT, 32x32]\n",
      "  %W_query.lora.A[FLOAT, 32x2]\n",
      "  %W_query.lora.B[FLOAT, 2x32]\n",
      "  %W_key.linear.weight[FLOAT, 32x32]\n",
      "  %W_key.lora.A[FLOAT, 32x2]\n",
      "  %W_key.lora.B[FLOAT, 2x32]\n",
      "  %W_value.linear.weight[FLOAT, 32x32]\n",
      "  %W_value.lora.A[FLOAT, 32x2]\n",
      "  %W_value.lora.B[FLOAT, 2x32]\n",
      "  %feed_forward.0.linear.weight[FLOAT, 128x32]\n",
      "  %feed_forward.0.linear.bias[FLOAT, 128]\n",
      "  %feed_forward.0.lora.A[FLOAT, 32x2]\n",
      "  %feed_forward.0.lora.B[FLOAT, 2x128]\n",
      "  %feed_forward.2.linear.weight[FLOAT, 32x128]\n",
      "  %feed_forward.2.linear.bias[FLOAT, 32]\n",
      "  %feed_forward.2.lora.A[FLOAT, 128x2]\n",
      "  %feed_forward.2.lora.B[FLOAT, 2x32]\n",
      "  %layer_norm1.weight[FLOAT, 32]\n",
      "  %layer_norm1.bias[FLOAT, 32]\n",
      "  %layer_norm2.weight[FLOAT, 32]\n",
      "  %layer_norm2.bias[FLOAT, 32]\n",
      "  %W_query.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %W_query.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %W_key.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %W_key.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %W_value.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %W_value.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %feed_forward.0.lora.A_grad.accumulation.buffer[FLOAT, 32x2]\n",
      "  %feed_forward.0.lora.B_grad.accumulation.buffer[FLOAT, 2x128]\n",
      "  %feed_forward.2.lora.A_grad.accumulation.buffer[FLOAT, 128x2]\n",
      "  %feed_forward.2.lora.B_grad.accumulation.buffer[FLOAT, 2x32]\n",
      "  %layer_norm1.weight_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm1.bias_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm2.weight_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %layer_norm2.bias_grad.accumulation.buffer[FLOAT, 32]\n",
      "  %lazy_reset_grad[BOOL, 1]\n",
      ") initializers (\n",
      "  %/W_key/Constant_output_0[FLOAT, scalar]\n",
      "  %/dropout/Constant_1_output_0[BOOL, scalar]\n",
      "  %/dropout_1/Constant_1_output_0[BOOL, scalar]\n",
      "  %/dropout/Constant_output_0[FLOAT, scalar]\n",
      "  %/Pow_output_0[FLOAT, scalar]\n",
      "  %onnx::loss::8_grad[FLOAT, scalar]\n",
      ") {\n",
      "  %/feed_forward/feed_forward.0/MatMul_output_0 = MatMul(%feed_forward.0.lora.A, %feed_forward.0.lora.B)\n",
      "  %/feed_forward/feed_forward.0/Transpose_output_0 = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.0/MatMul_output_0)\n",
      "  %/feed_forward/feed_forward.0/Mul_output_0 = Mul(%/feed_forward/feed_forward.0/Transpose_output_0, %/W_key/Constant_output_0)\n",
      "  %/feed_forward/feed_forward.0/Add_output_0 = Add(%feed_forward.0.linear.weight, %/feed_forward/feed_forward.0/Mul_output_0)\n",
      "  %/W_value/MatMul_output_0 = MatMul(%W_value.lora.A, %W_value.lora.B)\n",
      "  %/W_value/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_value/MatMul_output_0)\n",
      "  %/W_value/Mul_output_0 = Mul(%/W_value/Transpose_output_0, %/W_key/Constant_output_0)\n",
      "  %/W_value/Add_output_0 = Add(%W_value.linear.weight, %/W_value/Mul_output_0)\n",
      "  %/W_value/Transpose_1_output_0 = Transpose[perm = [1, 0]](%/W_value/Add_output_0)\n",
      "  %/W_value/MatMul_1_output_0 = MatMul(%input, %/W_value/Transpose_1_output_0)\n",
      "  %/W_query/MatMul_output_0 = MatMul(%W_query.lora.A, %W_query.lora.B)\n",
      "  %/W_query/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_query/MatMul_output_0)\n",
      "  %/W_query/Mul_output_0 = Mul(%/W_query/Transpose_output_0, %/W_key/Constant_output_0)\n",
      "  %/W_query/Add_output_0 = Add(%W_query.linear.weight, %/W_query/Mul_output_0)\n",
      "  %/W_query/Transpose_1_output_0 = Transpose[perm = [1, 0]](%/W_query/Add_output_0)\n",
      "  %/W_query/MatMul_1_output_0 = MatMul(%input, %/W_query/Transpose_1_output_0)\n",
      "  %/W_key/MatMul_output_0 = MatMul(%W_key.lora.A, %W_key.lora.B)\n",
      "  %/W_key/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_key/MatMul_output_0)\n",
      "  %/W_key/Mul_output_0 = Mul(%/W_key/Transpose_output_0, %/W_key/Constant_output_0)\n",
      "  %/W_key/Add_output_0 = Add(%W_key.linear.weight, %/W_key/Mul_output_0)\n",
      "  %/W_key/Transpose_1_output_0 = Transpose[perm = [1, 0]](%/W_key/Add_output_0)\n",
      "  %/W_key/MatMul_1_output_0 = MatMul(%input, %/W_key/Transpose_1_output_0)\n",
      "  %/Transpose_output_0 = Transpose[perm = [1, 0]](%/W_key/MatMul_1_output_0)\n",
      "  %/MatMul_output_0 = MatMul(%/W_query/MatMul_1_output_0, %/Transpose_output_0)\n",
      "  %/Div_output_0 = Div(%/MatMul_output_0, %/Pow_output_0)\n",
      "  %/Softmax_output_0 = Softmax[axis = -1](%/Div_output_0)\n",
      "  %/dropout/Dropout_output_0, %/dropout/Dropout_output_1 = Dropout(%/Softmax_output_0, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/MatMul_1_output_0 = MatMul(%/dropout/Dropout_output_0, %/W_value/MatMul_1_output_0)\n",
      "  %/Add_output_0 = Add(%input, %/MatMul_1_output_0)\n",
      "  %/layer_norm1/Add_1_output_0, %saved_mean, %saved_inv_std_var = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_output_0, %layer_norm1.weight, %layer_norm1.bias)\n",
      "  %/feed_forward/feed_forward.0/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/layer_norm1/Add_1_output_0, %/feed_forward/feed_forward.0/Add_output_0, %feed_forward.0.linear.bias)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0 = Relu(%/feed_forward/feed_forward.0/Gemm_output_0)\n",
      "  %/feed_forward/feed_forward.2/MatMul_output_0 = MatMul(%feed_forward.2.lora.A, %feed_forward.2.lora.B)\n",
      "  %/feed_forward/feed_forward.2/Transpose_output_0 = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.2/MatMul_output_0)\n",
      "  %/feed_forward/feed_forward.2/Mul_output_0 = Mul(%/feed_forward/feed_forward.2/Transpose_output_0, %/W_key/Constant_output_0)\n",
      "  %/feed_forward/feed_forward.2/Add_output_0 = Add(%feed_forward.2.linear.weight, %/feed_forward/feed_forward.2/Mul_output_0)\n",
      "  %/feed_forward/feed_forward.2/Gemm_output_0 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%/feed_forward/feed_forward.1/Relu_output_0, %/feed_forward/feed_forward.2/Add_output_0, %feed_forward.2.linear.bias)\n",
      "  %/dropout_1/Dropout_output_0, %/dropout_1/Dropout_output_1 = Dropout(%/feed_forward/feed_forward.2/Gemm_output_0, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %/Add_1_output_0 = Add(%/layer_norm1/Add_1_output_0, %/dropout_1/Dropout_output_0)\n",
      "  %output, %saved_mean_token_0, %saved_inv_std_var_token_1 = LayerNormalization[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/Add_1_output_0, %layer_norm2.weight, %layer_norm2.bias)\n",
      "  %onnx::loss::8, %onnx::log_prob::9 = SoftmaxCrossEntropyLoss[reduction = 'mean'](%output, %labels)\n",
      "  %output_grad = SoftmaxCrossEntropyLossGrad[reduction = 'mean'](%onnx::loss::8_grad, %onnx::log_prob::9, %labels)\n",
      "  %/Add_1_output_0_grad, %layer_norm2.weight_grad, %layer_norm2.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%output_grad, %/Add_1_output_0, %layer_norm2.weight, %saved_mean_token_0, %saved_inv_std_var_token_1)\n",
      "  %/dropout_1/Dropout_output_0_grad = Identity(%/Add_1_output_0_grad)\n",
      "  %/feed_forward/feed_forward.2/Gemm_output_0_grad = DropoutGrad(%/dropout_1/Dropout_output_0_grad, %/dropout_1/Dropout_output_1, %/dropout/Constant_output_0, %/dropout_1/Constant_1_output_0)\n",
      "  %/feed_forward/feed_forward.1/Relu_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%/feed_forward/feed_forward.2/Gemm_output_0_grad, %/feed_forward/feed_forward.2/Add_output_0)\n",
      "  %/feed_forward/feed_forward.0/Gemm_output_0_grad = ReluGrad(%/feed_forward/feed_forward.1/Relu_output_0_grad, %/feed_forward/feed_forward.1/Relu_output_0)\n",
      "  %/feed_forward/feed_forward.0/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/feed_forward/feed_forward.0/Gemm_output_0_grad, %/layer_norm1/Add_1_output_0)\n",
      "  %/feed_forward/feed_forward.0/Mul_output_0_grad = Identity(%/feed_forward/feed_forward.0/Add_output_0_grad)\n",
      "  %/feed_forward/feed_forward.0/Mul_Grad/PreReduceGrad0 = Mul(%/feed_forward/feed_forward.0/Mul_output_0_grad, %/W_key/Constant_output_0)\n",
      "  %/feed_forward/feed_forward.0/Transpose_output_0_grad = Identity(%/feed_forward/feed_forward.0/Mul_Grad/PreReduceGrad0)\n",
      "  %/feed_forward/feed_forward.0/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.0/Transpose_output_0_grad)\n",
      "  %feed_forward.0.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%feed_forward.0.lora.A, %/feed_forward/feed_forward.0/MatMul_output_0_grad)\n",
      "  %feed_forward.0.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/feed_forward/feed_forward.0/MatMul_output_0_grad, %feed_forward.0.lora.B)\n",
      "  %/feed_forward/feed_forward.2/Add_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/feed_forward/feed_forward.2/Gemm_output_0_grad, %/feed_forward/feed_forward.1/Relu_output_0)\n",
      "  %/feed_forward/feed_forward.2/Mul_output_0_grad = Identity(%/feed_forward/feed_forward.2/Add_output_0_grad)\n",
      "  %/feed_forward/feed_forward.2/Mul_Grad/PreReduceGrad0 = Mul(%/feed_forward/feed_forward.2/Mul_output_0_grad, %/W_key/Constant_output_0)\n",
      "  %/feed_forward/feed_forward.2/Transpose_output_0_grad = Identity(%/feed_forward/feed_forward.2/Mul_Grad/PreReduceGrad0)\n",
      "  %/feed_forward/feed_forward.2/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/feed_forward/feed_forward.2/Transpose_output_0_grad)\n",
      "  %feed_forward.2.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%feed_forward.2.lora.A, %/feed_forward/feed_forward.2/MatMul_output_0_grad)\n",
      "  %feed_forward.2.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/feed_forward/feed_forward.2/MatMul_output_0_grad, %feed_forward.2.lora.B)\n",
      "  %/layer_norm1/Add_1_output_0_grad_1 = Identity(%/Add_1_output_0_grad)\n",
      "  %/layer_norm1/Add_1_output_0_grad_0 = Gemm[alpha = 1, beta = 0, transA = 0, transB = 0](%/feed_forward/feed_forward.0/Gemm_output_0_grad, %/feed_forward/feed_forward.0/Add_output_0)\n",
      "  %/layer_norm1/Add_1_output_0_grad = Sum(%/layer_norm1/Add_1_output_0_grad_0, %/layer_norm1/Add_1_output_0_grad_1)\n",
      "  %/Add_output_0_grad, %layer_norm1.weight_grad, %layer_norm1.bias_grad = LayerNormalizationGrad[axis = -1, epsilon = 9.99999974737875e-06, stash_type = 1](%/layer_norm1/Add_1_output_0_grad, %/Add_output_0, %layer_norm1.weight, %saved_mean, %saved_inv_std_var)\n",
      "  %/MatMul_1_output_0_grad = Identity(%/Add_output_0_grad)\n",
      "  %/W_value/MatMul_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/dropout/Dropout_output_0, %/MatMul_1_output_0_grad)\n",
      "  %/W_value/Transpose_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_value/MatMul_1_output_0_grad)\n",
      "  %/W_value/Add_output_0_grad = Transpose[perm = [1, 0]](%/W_value/Transpose_1_output_0_grad)\n",
      "  %/W_value/Mul_output_0_grad = Identity(%/W_value/Add_output_0_grad)\n",
      "  %/W_value/Mul_Grad/PreReduceGrad0 = Mul(%/W_value/Mul_output_0_grad, %/W_key/Constant_output_0)\n",
      "  %/W_value/Transpose_output_0_grad = Identity(%/W_value/Mul_Grad/PreReduceGrad0)\n",
      "  %/W_value/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/W_value/Transpose_output_0_grad)\n",
      "  %W_value.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%W_value.lora.A, %/W_value/MatMul_output_0_grad)\n",
      "  %W_value.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/W_value/MatMul_output_0_grad, %W_value.lora.B)\n",
      "  %/dropout/Dropout_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/MatMul_1_output_0_grad, %/W_value/MatMul_1_output_0)\n",
      "  %/Softmax_output_0_grad = DropoutGrad(%/dropout/Dropout_output_0_grad, %/dropout/Dropout_output_1, %/dropout/Constant_output_0, %/dropout/Constant_1_output_0)\n",
      "  %/Div_output_0_grad = SoftmaxGrad_13[axis = -1](%/Softmax_output_0_grad, %/Softmax_output_0)\n",
      "  %/Div_Grad/PreReduceGrad0 = Div(%/Div_output_0_grad, %/Pow_output_0)\n",
      "  %/MatMul_output_0_grad = Identity(%/Div_Grad/PreReduceGrad0)\n",
      "  %/W_query/MatMul_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/MatMul_output_0_grad, %/Transpose_output_0)\n",
      "  %/W_query/Transpose_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_query/MatMul_1_output_0_grad)\n",
      "  %/W_query/Add_output_0_grad = Transpose[perm = [1, 0]](%/W_query/Transpose_1_output_0_grad)\n",
      "  %/W_query/Mul_output_0_grad = Identity(%/W_query/Add_output_0_grad)\n",
      "  %/W_query/Mul_Grad/PreReduceGrad0 = Mul(%/W_query/Mul_output_0_grad, %/W_key/Constant_output_0)\n",
      "  %/W_query/Transpose_output_0_grad = Identity(%/W_query/Mul_Grad/PreReduceGrad0)\n",
      "  %/W_query/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/W_query/Transpose_output_0_grad)\n",
      "  %W_query.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%W_query.lora.A, %/W_query/MatMul_output_0_grad)\n",
      "  %W_query.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/W_query/MatMul_output_0_grad, %W_query.lora.B)\n",
      "  %/Transpose_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%/W_query/MatMul_1_output_0, %/MatMul_output_0_grad)\n",
      "  %/W_key/MatMul_1_output_0_grad = Transpose[perm = [1, 0]](%/Transpose_output_0_grad)\n",
      "  %/W_key/Transpose_1_output_0_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%input, %/W_key/MatMul_1_output_0_grad)\n",
      "  %/W_key/Add_output_0_grad = Transpose[perm = [1, 0]](%/W_key/Transpose_1_output_0_grad)\n",
      "  %/W_key/Mul_output_0_grad = Identity(%/W_key/Add_output_0_grad)\n",
      "  %/W_key/Mul_Grad/PreReduceGrad0 = Mul(%/W_key/Mul_output_0_grad, %/W_key/Constant_output_0)\n",
      "  %/W_key/Transpose_output_0_grad = Identity(%/W_key/Mul_Grad/PreReduceGrad0)\n",
      "  %/W_key/MatMul_output_0_grad = Transpose[perm = [1, 0]](%/W_key/Transpose_output_0_grad)\n",
      "  %W_key.lora.B_grad = Gemm[alpha = 1, beta = 0, transA = 1, transB = 0](%W_key.lora.A, %/W_key/MatMul_output_0_grad)\n",
      "  %W_key.lora.A_grad = Gemm[alpha = 1, beta = 0, transA = 0, transB = 1](%/W_key/MatMul_output_0_grad, %W_key.lora.B)\n",
      "  %W_query.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%W_query.lora.A_grad.accumulation.buffer, %W_query.lora.A_grad, %lazy_reset_grad)\n",
      "  %W_query.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%W_query.lora.B_grad.accumulation.buffer, %W_query.lora.B_grad, %lazy_reset_grad)\n",
      "  %W_key.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%W_key.lora.A_grad.accumulation.buffer, %W_key.lora.A_grad, %lazy_reset_grad)\n",
      "  %W_key.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%W_key.lora.B_grad.accumulation.buffer, %W_key.lora.B_grad, %lazy_reset_grad)\n",
      "  %W_value.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%W_value.lora.A_grad.accumulation.buffer, %W_value.lora.A_grad, %lazy_reset_grad)\n",
      "  %W_value.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%W_value.lora.B_grad.accumulation.buffer, %W_value.lora.B_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.lora.A_grad.accumulation.buffer, %feed_forward.0.lora.A_grad, %lazy_reset_grad)\n",
      "  %feed_forward.0.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.0.lora.B_grad.accumulation.buffer, %feed_forward.0.lora.B_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.lora.A_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.lora.A_grad.accumulation.buffer, %feed_forward.2.lora.A_grad, %lazy_reset_grad)\n",
      "  %feed_forward.2.lora.B_grad.accumulation.out = InPlaceAccumulatorV2(%feed_forward.2.lora.B_grad.accumulation.buffer, %feed_forward.2.lora.B_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.weight_grad.accumulation.buffer, %layer_norm1.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm1.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm1.bias_grad.accumulation.buffer, %layer_norm1.bias_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.weight_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.weight_grad.accumulation.buffer, %layer_norm2.weight_grad, %lazy_reset_grad)\n",
      "  %layer_norm2.bias_grad.accumulation.out = InPlaceAccumulatorV2(%layer_norm2.bias_grad.accumulation.buffer, %layer_norm2.bias_grad, %lazy_reset_grad)\n",
      "  return %onnx::loss::8, %output, %W_query.lora.A_grad.accumulation.out, %W_query.lora.B_grad.accumulation.out, %W_key.lora.A_grad.accumulation.out, %W_key.lora.B_grad.accumulation.out, %W_value.lora.A_grad.accumulation.out, %W_value.lora.B_grad.accumulation.out, %feed_forward.0.lora.A_grad.accumulation.out, %feed_forward.0.lora.B_grad.accumulation.out, %feed_forward.2.lora.A_grad.accumulation.out, %feed_forward.2.lora.B_grad.accumulation.out, %layer_norm1.weight_grad.accumulation.out, %layer_norm1.bias_grad.accumulation.out, %layer_norm2.weight_grad.accumulation.out, %layer_norm2.bias_grad.accumulation.out\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"Transformer_Lora/All_layers/training_model_all.onnx\")\n",
    "print('Model :\\n\\n{}'.format(onnx.helper.printable_graph(model.graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96eaffd5-f30d-41e2-9366-1cb222fdfe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerBlock(\n",
      "  (W_query): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (W_key): Linear(in_features=32, out_features=32, bias=False)\n",
      "  (W_value): LinearWithLoRAMerged(\n",
      "    (linear): Linear(in_features=32, out_features=32, bias=False)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (feed_forward): Sequential(\n",
      "    (0): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b26845-10df-4b42-ab96-9276c5c1ecea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
